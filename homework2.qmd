---
title: "Homerwork 2"
author: "Notis Lapatas"
date: 2023-05-21
format: 
  docx: default
  html:
    toc: true
    toc_float: true
    code-fold: true
editor: visual
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(skimr)
library(countrycode)
library(here)
library(forecast)
```

# Data Visualisation - Exploration

Now that you've demonstrated your software is setup, and you have the basics of data manipulation, the goal of this assignment is to practice transforming, visualising, and exploring data.

# Mass shootings in the US

In July 2012, in the aftermath of a mass shooting in a movie theater in Aurora, Colorado, [Mother Jones](https://www.motherjones.com/politics/2012/07/mass-shootings-map/) published a report on mass shootings in the United States since 1982. Importantly, they provided the underlying data set as [an open-source database](https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/) for anyone interested in studying and understanding this criminal behavior.

## Obtain the data

```{r}
#| echo: false
#| message: false
#| warning: false

mass_shootings <- readr::read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQBEbQoWMn_P81DuwmlQC0_jr2sJDzkkC0mvF6WLcM53ZYXi8RMfUlunvP1B5W0jRrJvH-wc-WGjDB1/pub?gid=0&single=true&output=csv')

glimpse(mass_shootings)
```

| column(variable)     | description                                                                 |
|-------------------|-----------------------------------------------------|
| case                 | short name of incident                                                      |
| year, month, day     | year, month, day in which the shooting occurred                             |
| location             | city and state where the shooting occcurred                                 |
| summary              | brief description of the incident                                           |
| fatalities           | Number of fatalities in the incident, excluding the shooter                 |
| injured              | Number of injured, non-fatal victims in the incident, excluding the shooter |
| total_victims        | number of total victims in the incident, excluding the shooter              |
| location_type        | generic location in which the shooting took place                           |
| male                 | logical value, indicating whether the shooter was male                      |
| age_of_shooter       | age of the shooter when the incident occured                                |
| race                 | race of the shooter                                                         |
| prior_mental_illness | did the shooter show evidence of mental illness prior to the incident?      |

## Explore the data

### Specific questions

-   Generate a data frame that summarizes the number of mass shootings per year.

```{r}
#creating a data frame that contains the number of mass shootings per year
mass_shootings_per_year <- mass_shootings %>% 
  group_by(year) %>% 
  summarise('No. of mass shootings'=n())
```

-   Generate a bar chart that identifies the number of mass shooters associated with each race category. The bars should be sorted from highest to lowest and each bar should show its number.

```{r}
#cleaning the data
mass_shootings <- mass_shootings %>% 
  mutate(race  = case_when(race %in% c("white") ~ "White", TRUE ~ race)) 
#creating the plot
mass_shootings %>% 
  mutate(race = fct_rev(fct_infreq(race))) %>%
  ggplot(aes(y=race))+
  geom_bar()+
  theme_minimal(base_size=6)
```

-   Generate a boxplot visualizing the number of total victims, by type of location.

```{r}
#cleaning the data (location 8 variable)
mass_shootings <- mass_shootings %>% 
  mutate(location...8  = case_when(location...8 %in% c("workplace") ~ "Workplace",
                                   location...8 %in% c("\nWorkplace") ~ "Workplace",
                                   location...8 %in% c("Other\n") ~ "Other",
                                   TRUE ~ location...8)) 
#counting no of mass shootings by category and creating the plot
mass_shootings %>% 
  group_by(location...8) %>% 
  summarise('Sum'=n()) %>% 
  ggplot(aes(x=location...8, y=Sum))+
  geom_boxplot()+
  labs(x="Location")
```

-   Redraw the same plot, but remove the Las Vegas Strip massacre from the dataset.

```{r}
#making a better plot 
mass_shootings %>% 
  mutate(location...8 = fct_rev(fct_infreq(location...8))) %>%
  ggplot(aes(y=location...8))+
  geom_bar()+
  labs(y="Location", title = "Workplace is the most dangerous place for mass shootings to occur")+
  theme_minimal(base_size=6)
```

### More open-ended questions

Address the following questions. Generate appropriate figures/tables to support your conclusions.

-   How many white males with prior signs of mental illness initiated a mass shooting after 2000?

```{r}
#cleaning the data (gender and prior_signs_mental_health_issues variables)
mass_shootings <- mass_shootings %>% 
  mutate(gender  = case_when(gender %in% c("M") ~ "Male",
                                   TRUE ~ gender)) %>% 
  mutate(prior_signs_mental_health_issues  = case_when(prior_signs_mental_health_issues %in% c("yes") ~ "Yes",
                                   TRUE ~ prior_signs_mental_health_issues)) 

#
mass_shootings %>% 
  filter(race=="White" 
         & gender == "Male" 
         & year>2000 
         & prior_signs_mental_health_issues=="Yes") %>% 
  count()
```
25  white males with prior signs of mental illness initiated a mass shooting after 2000.

-   Which month of the year has the most mass shootings? Generate a bar chart sorted in chronological (natural) order (Jan-Feb-Mar- etc) to provide evidence of your answer.

```{r}
#transforming character to date 
mass_shootings <-mass_shootings %>% 
  mutate(date = as.Date(date, format = "%m/%d/%y"))
#calxulating the shootings per month 
shootings_month <- mass_shootings %>% 
  group_by(month(date)) %>% 
  summarise('shootings_per_month'=n()) %>% 
  rename("month" = `month(date)`) %>% 
  #adding a column with the months abbriviations
  mutate('m'=month.abb) 
  
#finding the month with the most shootings
shootings_month %>% 
  slice_max(shootings_per_month)

#creating bar chart
shootings_month %>%
  mutate(m = fct_reorder(m,month)) %>% 
  ggplot(aes(x=m, y=shootings_per_month))+
  geom_col()+
  labs(y= "No. of shootings", title = "Seasonality even in shootings")
```
The months with the most shootings are February, June and November.


-   How does the distribution of mass shooting fatalities differ between White and Black shooters? What about White and Latino shooters?

```{r}
#creating a data frame that contains black, white and the number fatalities in each incident 
r <- mass_shootings %>% 
  filter(race == 'White' | race=='Black') %>% 
  group_by(race) %>% 
  summarise('fatalities' = fatalities)

#conducting a t-test
t.test(r$fatalities~r$race)
#counting the number of total fatalities 
r %>% 
  summarise(sum=sum(fatalities))
#counting the number of incidents by race
r %>% 
  count()

#creating a data frame that contains latino, white and the number fatalities in each incident
r2 <- mass_shootings %>% 
  filter(race == 'White' | race=='Latino') %>% 
  group_by(race) %>% 
  summarise('fatalities' = fatalities)
#conducting a t-test
t.test(r2$fatalities~r2$race)
#counting the number of total fatalities 
r2 %>% 
  summarise(sum=sum(fatalities))
#counting the number of incidents by race
r2 %>% 
  count()

#creating a data frame  that contains latino, white, black, the number 
#fatalities in each incident and the date
r_all <- mass_shootings %>% 
  filter(race == 'White' | race=='Black' |  race=='Latino') %>% 
  group_by(race) %>% 
  summarise('fatalities' = fatalities, 'date' = date)
#visualizing data
ggplot(r_all, aes(x=date, y=fatalities, color=race))+
  geom_smooth()
```
Black and white
By conducting a t-test we can identify that there is a difference between the mean number of fatalities of shootings caused by black people vs white. to be more specific if the race is black the average number of fatalities is 4.93 and if the race is white the number raises to 8.42. The difference is statistically important with t = -3.0091 and p-value = 0.003736. In addition Black people were involved in 16 incidents and caused 79 fatalities while white people were involved in 73 incidents and caused 632 fatalities.

Latino and white
The difference is not statistically important at a 95 percent confidence interval with t = -1.3857 and p-value = 0.1805. Latino people were involved in 12 incidents and caused 79 fatalities while white people were involved in 75 incidents and caused 632 fatalities. If the race is latino the average number of fatalities is 6.08 and if the race is white the number raises to 8.42.

In order to understand the data better we create the above plot. As we can see the number of fatalities caused by white people is pretty stable over time while the number is declining in the case of black people. Regarding latino people we can identify a big spike in 2022 but over the number of fatalities is quite similar to that of white people.

### Very open-ended

-   Are mass shootings with shooters suffering from mental illness different from mass shootings with no signs of mental illness in the shooter?

```{r}
#checking the mental health status if it contains the word mental and 
#creating a new column with the mental health status
mental <- mass_shootings %>% 
  mutate(Mental_illness = case_when(
    grepl("mental",mental_health_details) ~ "yes",
    mental_health_details == "-" ~"Not available",
    TRUE ~"no")) %>% 
  #filtering only for the cases that we have data 
  filter(Mental_illness =='yes' | Mental_illness =='no')

#conducting a t-test
t.test(mental$fatalities~mental$Mental_illness)
#conducting a t-test
t.test(mental$injured~mental$Mental_illness)
#conducting a t-test
t.test(mental$total_victims~mental$Mental_illness)

#finding how many mental illness cases we have
lenght <- mental %>% 
  group_by(Mental_illness) %>% 
  summarise(percent = n())

#assigning the number of incidents with and without mental illness
l_no <- lenght[[1,2]]
l_yes <- lenght[[2,2]]

#calculating the percentage of the incidents by race
mental %>% 
  group_by(race, Mental_illness) %>% 
  summarise('count' = n()) %>% 
  mutate('percent' = case_when(Mental_illness == "no" ~ count/l_no,Mental_illness == "yes" ~ count/l_yes))

#calculating the percentage of the incidents by location
mental %>% 
  group_by(location...8, Mental_illness) %>% 
  summarise('count' = n()) %>% 
  mutate('percent' = case_when(Mental_illness == "no" ~ count/l_no,Mental_illness == "yes" ~ count/l_yes))
```
We were unable to find a conclusive and statistically important difference between the mass shootings conducted by shooters suffering from mental illness and those who weren't. To be more specific there was no difference in the number of fatalities, injuries and total victims. It seems thought that white people that conduct shootings are mostly mentally ill while latino and black people aren't. In addition, it seems that if the shooting takes place on a religious place or in a school it's more likely that the shooter is mentally ill while that's not the case if it takes place on a military location.

-   Assess the relationship between mental illness and total victims, mental illness and location type, and the intersection of all three variables.

```{r}
#Chi-sq test
chisq.test(mental$Mental_illness, mental$location...8, correct=FALSE)
#creating a data frame that has the number of total victims by type of location and mental illness status
mental_loc_victims <- mental %>% 
  group_by(location...8, Mental_illness) %>% 
  summarise('Total_victims' = sum(total_victims)) 
#conducting an Anova
two.way <- aov(Total_victims ~ Mental_illness + location...8, data = mental_loc_victims)
summary(two.way)
```

The analysis was conducted in the previous chunk of code, there is no statistically important difference between the total victims conducted by shooters suffering from mental illness and those who weren't. t = -0.8569, p-value = 0.3938 and the mean number of total victims for shooters that weren't suffering from mental illness is 8.171875 while for those who did is 9.555556.
Also it seems that if the shooting takes place on a religious place or in a school it's more likely that the shooter is mentally ill while that's not the case if it takes place on a military location. But we don't enough enough data to conduct a conclusive Chi-square analysis to identify the link between the two categorical variables (with our data it seems that they are independent). 
In order to identify the link among the mental status of the shooter, the location type and the total victims, we conduct an Anova. We identify that the p-values are not statistically significant at a 95% confidence level (Mental illness p-value = 0.466, location type p-value = 0.231) so there is no link among these variables.

Make sure to provide a couple of sentences of written interpretation of your tables/figures. Graphs and tables alone will not be sufficient to answer this question.

# Exploring credit card fraud

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no? (well, not quite as we will see later in the course)

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0 and save it in your `dsb` repo, under the `data` folder

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv"))

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

-   In this dataset, how likely are fraudulent transactions? Generate a table that summarizes the number and frequency of fraudulent transactions per year.

```{r}
card_fraud %>% 
  group_by(trans_year) %>% 
  summarise('frauds' = sum(is_fraud),'total' = n()) %>% 
  mutate('percentage'=frauds/total)
```
In 2019 2.721 transactions were fraudulent 0.5685%
In 2020 1215 transactions were fraudulent 0.6316%

-   How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms.

```{r}
#calculating the number of fraudulent transactions, the total number 
#of transactions and the % of fraudulent transactions
card_fraud %>% 
  group_by(trans_year,is_fraud) %>% 
  summarise('fraudulent' = sum(amt)) %>% 
  ungroup() %>% 
  group_by(trans_year) %>% 
  mutate('total'=sum(fraudulent)) %>% 
  filter(is_fraud==1) %>% 
  mutate('percentage'=fraudulent/total) %>% 
  select(-is_fraud)
```

-   Generate a histogram that shows the distribution of amounts charged to credit card, both for legitimate and fraudulent accounts. Also, for both types of transactions, calculate some quick summary statistics.

```{r}
#adding a column (non numerical) so we can identify if it's fraud or not 
card_fraud <- card_fraud %>% 
  mutate(fraud= case_when(is_fraud==1~"Yes",is_fraud==0~"No"))
#creating the plot
ggplot(card_fraud, aes(x=amt, color=fraud))+
  geom_histogram(fill="white", alpha=0.5, position="identity", bins=100)+
  xlim(0,1000)

#creating a data frame that contains all legitimate transactions 
legit <- card_fraud %>% 
  filter(is_fraud==0)
#creating a data frame that contains all fraudulent transactions 
fraud <- card_fraud %>% 
  filter(is_fraud==1)
#calculating summary statistics for legitimate transactions 
skimr::skim(legit)
#calculating summary statistics for fraudulent transactions 
skimr::skim(fraud)
```
We limit the plot x axis to 1000$ so it is easier to read. Note that the transactions over 500$ uo to 27119.77$ (highest amount in this data set) are fraudulent.


-   What types of purchases are most likely to be instances of fraud? Consider category of merchants and produce a bar chart that shows % of total fraudulent transactions sorted in order.

```{r}
card_fraud %>%
  group_by(category) %>% 
  summarise('frauds' = sum(is_fraud),'total' = n()) %>% 
  mutate('percentage'=frauds/sum(frauds)) %>% 
  mutate(category = fct_reorder(category,percentage)) %>%
  ggplot(aes(x=percentage, y=category))+
  geom_col()+
  scale_x_continuous(labels = scales::percent)+
  theme_minimal(base_size=6)
```

-   When is fraud more prevalent? Which days, months, hours? To create new variables to help you in your analysis, we use the `lubridate` package and the following code

<!-- -->  

    mutate(
      date_only = lubridate::date(trans_date_trans_time),
      month_name = lubridate::month(trans_date_trans_time, label=TRUE),
      hour = lubridate::hour(trans_date_trans_time),
      weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
      )

-   Are older customers significantly more likely to be victims of credit card fraud? To calculate a customer's age, we use the `lubridate` package and the following code

<!-- -->

      mutate(
       age = interval(dob, trans_date_trans_time) / years(1),
        )

```{r}
#adding columns month, hour and weekday
card_fraud <- card_fraud %>%
  mutate(
      date_only = lubridate::date(trans_date_trans_time),
      month_name = lubridate::month(trans_date_trans_time, label=TRUE),
      hour = lubridate::hour(trans_date_trans_time),
      weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
      )

#finding the months with the most fraudulent transactions 
card_fraud %>%
  filter(is_fraud==1) %>% 
  group_by(month_name) %>% 
  summarise('per_month' = n()) %>% 
  slice_max(per_month)

#finding the hours with the most fraudulent transactions 
card_fraud %>%
  filter(is_fraud==1) %>% 
  group_by(hour) %>% 
  summarise('per_hour' = n()) %>% 
  slice_max(per_hour)

#finding the weekdays with the most fraudulent transactions 
card_fraud %>%
  filter(is_fraud==1) %>% 
  group_by(weekday) %>% 
  summarise('per_weekday' = n()) %>% 
  slice_max(per_weekday)

#calculating customer's age
card_fraud <- card_fraud %>%
  mutate(age = interval(dob, trans_date_trans_time) / years(1),)

#testing if there is a link between age and fraud 
t.test(card_fraud$age~card_fraud$fraud)
  
```
The months with the most fraudulent transactions are March and May
The hour with the most fraudulent transactions is 23:00
The weekdays with the most fraudulent transactions is Monday

By conducting a t-test analysis we can identify that older people are more likely to be victims of fraud. With a p-value < 2.2e-16 and t = -10.04 the difference is statistically significant at a 99% confidence level.

-   Is fraud related to distance? The distance between a card holder's home and the location of the transaction can be a feature that is related to fraud. To calculate distance, we need the latidue/longitude of card holders's home and the latitude/longitude of the transaction, and we will use the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) to calculate distance. I adapted code to [calculate distance between two points on earth](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/) which you can find below

```{r}
# distance between card holder's home and transaction
# code adapted from https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/


card_fraud <- card_fraud %>%
  mutate(
    
    # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),

    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))

  )

#testing if there is a link between distance and fraud instances
t.test(card_fraud$distance_km~card_fraud$fraud)
```
By conducting a t-test we can identify that there is no link between distance and fraud instances p-value = 0.969.

Plot a boxplot or a violin plot that looks at the relationship of distance and `is_fraud`. Does distance seem to be a useful feature in explaining fraud?

# Exploring sources of electricity production, CO2 emissions, and GDP per capita.

There are many sources of data on how countries generate their electricity and their CO2 emissions. I would like you to create three graphs:

## 1. A stacked area chart that shows how your own country generated its electricity since 2000.

You will use

`geom_area(colour="grey90", alpha = 0.5, position = "fill")`

## 2. A scatter plot that looks at how CO2 per capita and GDP per capita are related

## 3. A scatter plot that looks at how electricity usage (kWh) per capita/day GDP per capita are related

We will get energy data from the Our World in Data website, and CO2 and GDP per capita emissions from the World Bank, using the `wbstats`package.

```{r}
#| message: false
#| warning: false

# Download electricity data
url <- "https://nyc3.digitaloceanspaces.com/owid-public/data/energy/owid-energy-data.csv"

energy <- read_csv(url) %>% 
  filter(year >= 1990) %>% 
  drop_na(iso_code) %>% 
  select(1:3,
         biofuel = biofuel_electricity,
         coal = coal_electricity,
         gas = gas_electricity,
         hydro = hydro_electricity,
         nuclear = nuclear_electricity,
         oil = oil_electricity,
         other_renewable = other_renewable_exc_biofuel_electricity,
         solar = solar_electricity,
         wind = wind_electricity, 
         electricity_demand,
         electricity_generation,
         net_elec_imports,	# Net electricity imports, measured in terawatt-hours
         energy_per_capita,	# Primary energy consumption per capita, measured in kilowatt-hours	Calculated by Our World in Data based on BP Statistical Review of World Energy and EIA International Energy Data
         energy_per_gdp,	# Energy consumption per unit of GDP. This is measured in kilowatt-hours per 2011 international-$.
         per_capita_electricity, #	Electricity generation per capita, measured in kilowatt-hours
  ) 

# Download data for C02 emissions per capita https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         co2percap = value)


# Download data for GDP per capita  https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD
gdp_percap <- wb_data(country = "countries_only", 
                      indicator = "NY.GDP.PCAP.PP.KD", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         GDPpercap = value)
```

Specific questions:

1.  How would you turn `energy` to long, tidy format?
2.  You may need to join these data frames
    -   Use `left_join` from `dplyr` to [join the tables](http://r4ds.had.co.nz/relational-data.html)
    -   To complete the merge, you need a unique *key* to match observations between the data frames. Country names may not be consistent among the three dataframes, so please use the 3-digit ISO code for each country
    -   An aside: There is a great package called [`countrycode`](https://github.com/vincentarelbundock/countrycode) that helps solve the problem of inconsistent country names (Is it UK? United Kingdon? Great Britain?). `countrycode()` takes as an input a country's name in a specific format and outputs it using whatever format you specify.
3.  Write a function that takes as input any country's name and returns all three graphs. You can use the `patchwork` package to arrange the three graphs as shown below

![](images/electricity-co2-gdp.png)

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed Quarto Markdown (qmd) file as a Word document (use the "Render" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing tour changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: nobody
-   Approximately how much time did you spend on this problem set: way way too much
-   What, if anything, gave you the most trouble: not the problem sets, the limited time 

**Please seek out help when you need it,** and remember the [15-minute rule](https://mam2022.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
