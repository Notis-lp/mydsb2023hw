---
title: "Homework 3: Databases, web scraping, and a basic Shiny app"
author: "Notis Lapatas"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(tictoc)
library(skimr)
library(countrycode)
library(here)
library(DBI)
library(dbplyr)
library(arrow)
library(rvest)
library(robotstxt) # check if we're allowed to scrape the data
library(scales)
library(sf)
library(readxl)
library(ggrepel)
```

# Money in UK politics

## Open a connection to the database

The database made available by Simon Willison is an `SQLite` database

```{r}
sky_westminster <- DBI::dbConnect(
  drv = RSQLite::SQLite(),
  dbname = here::here("data", "sky-westminster-files.db")
)
```

How many tables does the database have?

```{r}
DBI::dbListTables(sky_westminster)

```
It has 7 tables.

## Which MP has received the most amount of money? 

```{r}
#creating tables 
payments_db <- dplyr::tbl(sky_westminster, "payments")
members_db <- dplyr::tbl(sky_westminster, "members")
#calculating the amount each MP has receive
pay_sum <- payments_db %>% 
  group_by(member_id) %>% 
  summarise('amount' = sum(value)) 
#joining the two tables
pay_sum <- left_join(pay_sum,members_db,by=c('member_id'='id')) 
#finding the name of the MP has received the most amount of money
name <- pay_sum %>% 
  slice_max(amount) %>% 
  select(name) %>% 
  pull
```
Theresa May has received the most money.

## Any `entity` that accounts for more than 5% of all donations?

```{r}
#we don't have to bring it into memory but because its a small table we might as well
payments_db<- payments_db %>%  collect()
#splitting date
payments_db <- payments_db %>% 
  mutate(month = word(date,start=3,end=3,sep=fixed(" ")),
         year = as.numeric(word(date,start=4,end=4,sep=fixed(" "))))
#finding if there is an entity that gave more than 5%
payments_db %>% 
  group_by(entity) %>% 
  summarise('amount' = sum(value)) %>% 
  mutate('percentage'=amount/sum(amount)) %>% 
  filter(percentage>=0.05) %>% 
  select(entity)
#finding the member id 
id_m <- payments_db %>% 
  filter(entity == 'Withers LLP') %>% 
  group_by(member_id) %>% 
  count() %>% 
  select(member_id) %>% 
  pull()
#using the id to find the name of the mp
members_db %>% 
  filter(id==id_m) %>% 
  select(name)
  
```
Yes there is, and it's Withers LLP. They gave a total of Â£1.812.732 (5,25%) to Sir Geoffrey Cox.

## Do `entity` donors give to a single party or not?

- How many distinct entities who paid money to MPS are there?
- How many (as a number and %) donated to MPs belonging to a single party only?
```{r}
#finding how many distinct entities exist
(dist_ent <- payments_db %>% 
  group_by(entity) %>% 
  count() %>% 
  nrow())
#bringing it to memory 
members_db <- members_db %>% collect()
#joining
ent_pay <- left_join(payments_db,members_db,by=c('member_id'='id')) 
#finding in how many parties each entity has contributed and counting 
#how many contributed to a single party only
(single <- ent_pay %>% 
  group_by(entity) %>% 
  count(party_id) %>% 
  count(entity) %>% 
  filter(n==1) %>% 
  nrow)
#calculating the percentage
single/dist_ent
```
There are 2213 distinct entities.
2036 entities donated to MPs belonging to a single party only, that's 92%


## Which party has raised the greatest amount of money in each of the years 2020-2022? 
```{r}
#creating table parties_db
parties_db <- dplyr::tbl(sky_westminster, "parties") %>%  collect()
#joining the tables 
ent_pay <- left_join(ent_pay,parties_db,by=c('party_id'='id')) 
#calculating the amount raised each year by each party 
per_party <- ent_pay %>% 
  filter(year>2019) %>% 
  group_by(year,name.y) %>% 
  summarise('total_year_donations'=sum(value)) %>% 
  group_by(year) %>% 
  mutate('prop'=total_year_donations/sum(total_year_donations)) %>% 
  rename('name'=name.y)
#finding the max for each year
per_party%>% 
  group_by(year) %>% 
  filter(total_year_donations==max(total_year_donations))
per_party
```
The conservative party raised each year (2020-2022) the greatest amount of money.

```{r}
#reordering and creating the plot
per_party %>% mutate(name = fct_rev(fct_reorder(name,total_year_donations))) %>% 
ggplot(aes(x=year, y=total_year_donations, fill=name))+
  geom_bar(stat='identity',position=position_dodge())+
  scale_y_continuous(labels = scales::dollar)+
  labs(title ="Conservatives have captured the majority of political demands", 
       subtitle = "Donations to political parties, 2020-2022")+
  xlab(NULL)+
  ylab(NULL)+
  theme(plot.subtitle = element_text(size = 7),
  axis.title = element_text(size = 7),
  plot.title = element_text(size = 10),
  legend.text = element_text(size = 7),
  legend.title = element_text(size = 8)) +labs(fill = "Party")
  theme_bw()
```


```{r}
#disconnecting from data base
dbDisconnect(sky_westminster)
```


# Anonymised Covid patient data from the CDC

## Obtain the data

```{r}
#| echo: false
#| message: false
#| warning: false


tic() # start timer
cdc_data <- open_dataset(here::here("data", "cdc-covid-geography"))
toc() # stop timer


glimpse(cdc_data)
```

```{r}
#collecting the data we nned from the database
d <- cdc_data %>% 
  filter((icu_yn=="Yes" | icu_yn=="No") & 
           (death_yn=="Yes" | death_yn=="No") & 
           (sex=="Male" | sex=="Female") & 
           (age_group!="Missing")) %>% 
  group_by(age_group,sex,icu_yn,death_yn) %>% 
  count() %>% 
  collect()
#calculating the percentage for each category
d <- d %>% 
  group_by(age_group,sex,icu_yn) %>% 
  mutate('total'=sum(n)) %>% 
  rename('deaths'=n) %>% 
  filter(death_yn=='Yes') %>% 
  mutate('perc'=deaths/total) 

d<- d %>% 
  mutate(icu_yn = case_when(
    icu_yn %in% c("Yes") ~ "ICU Admission",
    icu_yn %in% c("No") ~ "No ICU Admission", TRUE~icu_yn))
#plotting
ggplot(d,aes(x=perc, y=age_group))+
  geom_bar(stat="identity", position = "dodge", fill='tomato')+
  facet_grid(icu_yn~sex)+
  scale_x_continuous(labels = scales::percent)+
  labs(title ="Covid CFR % per age group, sex and ICU Admission", caption="Source CDC")+
  xlab(NULL)+
  ylab(NULL)+
    geom_text(aes(label = round(perc,2)*100),
             hjust = 1, 
             colour = "black", 
             size = 3)+
  theme(plot.caption = element_text(size = 6),
    axis.text = element_text(size = 7), axis.text.x = element_text(size = 7),
    plot.title = element_text(size = 10)) +labs(size = 8)+
  theme_light()

  
  
```

```{r warning=FALSE}
#gathering data and converting case_month to date
w <- cdc_data %>% 
  filter((icu_yn=="Yes" | icu_yn=="No") & 
           (death_yn=="Yes" | death_yn=="No") & 
           (sex=="Male" | sex=="Female") & 
           (age_group!="Missing")) %>%
  mutate(case_month = as.Date(paste(case_month, "-01", sep=""))) %>% 
  group_by(case_month,age_group,sex,icu_yn,death_yn) %>%
  count() %>% 
  collect()

#calculating the percentage for each category
w <- w %>% 
  group_by(case_month,age_group,sex,icu_yn) %>% 
  mutate('total'=sum(n)) %>% 
  rename('deaths'=n) %>% 
  filter(death_yn=='Yes') %>% 
  mutate('perc'=deaths/total) 

#renaming column icy_yn values
w<- w %>% 
  filter(case_month>'2020-02-01') %>% 
  mutate(icu_yn = case_when(
    icu_yn %in% c("Yes") ~ "ICU Admission",
    icu_yn %in% c("No") ~ "No ICU Admission", TRUE~icu_yn))

#plotting
ggplot(w,aes(x=case_month, y=perc, color=age_group))+
  geom_line()+
  facet_grid(icu_yn~sex, scales = "free")+
  scale_y_continuous(labels = scales::percent)+
  labs(title ="Covid CFR % per age group, sex and ICU Admission", caption="Source CDC", color= "Age Group")+
  xlab(NULL)+
  ylab(NULL)+
  geom_text_repel(aes(label = round(perc,2)*100), 
                  box.padding   = 0.15, 
                  point.padding = 0.2,size=2)+
  theme(plot.caption = element_text(size = 6),
    axis.text = element_text(size = 7), axis.text.x = element_text(size = 7),
    plot.title = element_text(size = 10)) +labs(size = 8)+
  theme_light()
```


```{r}
urban_rural <- read_xlsx(here::here("data", "NCHSURCodes2013.xlsx")) %>% 
  janitor::clean_names() 
```


Can you query the database, extract the relevant information, and reproduce the following two graphs that look at the Case Fatality ratio (CFR) in different counties, according to their population?


```{r}
#gathering data and converting case_month to date
temp <- cdc_data %>% 
  filter((death_yn=="Yes" | death_yn=="No")) %>% 
  mutate(case_month = as.Date(paste(case_month, "-01", sep=""))) %>% 
  group_by(case_month,county_fips_code,death_yn) %>%
  count() %>% 
  collect()

temp <- temp %>% drop_na()

urban_rural2 <- urban_rural %>% 
  filter(!is.na(x2013_code)) %>% 
  mutate(type = case_when(
    x2013_code == 1 ~ "1. Large central metro",
    x2013_code == 2 ~ "2. Large fringe metron",
    x2013_code == 3 ~ "3. Medium metro",
    x2013_code == 4 ~ "4. Small metropolitan",
    x2013_code == 5 ~ "5. Micropolitan",
    x2013_code == 6 ~ "6. Noncore",
    ))

  
temp2 <- left_join(temp,urban_rural2,by=c('county_fips_code'='fips_code'))
#calculating the percentage for each category
temp2 <- temp2 %>% 
  filter(case_month>'2020-01-01') %>% 
  group_by(case_month,type,death_yn) %>% 
  summarise('s'=sum(n)) %>% 
  ungroup() %>% 
  group_by(case_month,type) %>% 
  mutate('total'=sum(s)) %>% 
  rename('deaths'=s) %>% 
  filter(death_yn=='Yes') %>% 
  mutate('perc'=deaths/total) %>%
  drop_na()

#plotting
ggplot(temp2,aes(x=case_month, y=perc, color=type))+
  geom_line()+
  facet_wrap(~type, scales="free")+
  scale_y_continuous(labels = scales::percent)+
  labs(title ="Covid CFR % by county population", caption="Source CDC")+
  xlab(NULL)+
  ylab(NULL)+
  geom_text_repel(aes(label = round(perc,2)*100), 
                  box.padding   = 0.15, 
                  point.padding = 0.2,size=2)+
  theme(axis.title = element_text(size = 7),
    axis.text = element_text(size = 3), axis.text.x = element_text(size = 3),
    plot.title = element_text(size = 8),
    legend.text = element_text(size = 5)) +labs(size = 55) +
  theme_light()+
  theme(legend.position = "none")

```

```{r}
urban_rural <- urban_rural %>% 
  filter(!is.na(x2013_code)) %>% 
  mutate(type = case_when(
    x2013_code == 1 ~ "Urban",
    x2013_code == 2 ~ "Urban",
    x2013_code == 3 ~ "Urban",
    x2013_code == 4 ~ "Urban",
    x2013_code == 5 ~ "Rural",
    x2013_code == 6 ~ "Rural",
    ))

temp3 <- left_join(temp,urban_rural,by=c('county_fips_code'='fips_code'))

#calculating the percentage for each category
temp3 <- temp3 %>% 
  filter(case_month>'2020-01-01') %>% 
  group_by(case_month,type,death_yn) %>% 
  summarise('s'=sum(n)) %>% 
  ungroup() %>% 
  group_by(case_month,type) %>% 
  mutate('total'=sum(s)) %>% 
  rename('deaths'=s) %>% 
  filter(death_yn=='Yes') %>% 
  mutate('perc'=deaths/total) %>% 
  drop_na()

ggplot(temp3,aes(x=case_month, y=perc, color=type))+
  geom_line()+
  scale_y_continuous(labels = scales::percent)+
  labs(title ="Covid CFR % by county population", caption="Source CDC", color="Counties")+
  xlab(NULL)+
  ylab(NULL)+
  geom_text_repel(aes(label = round(perc,2)*100), 
                  box.padding   = 0.15, 
                  point.padding = 0.2,
                  size=2,
                  color="black")+
  theme(axis.title = element_text(size = 7),
    axis.text = element_text(size = 3), axis.text.x = element_text(size = 3),
    plot.title = element_text(size = 8),
    legend.text = element_text(size = 5)) +labs(size = 55) +
  theme_light()

```
# Money in US politics

```{r, eval=FALSE}
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false

library(robotstxt)
paths_allowed("https://www.opensecrets.org")

base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"

contributions_tables <- base_url %>%
  read_html() %>% 
  html_nodes(css="table") %>% # this will isolate all tables on page
  html_table()
```

```{r}
#cleaning names
contributions <- contributions_tables[[1]] %>% 
  janitor::clean_names()
```

 
```{r, eval=FALSE}
# function to parse_currency
parse_currency <- function(x){
  x %>%
    # remove dollar signs
    str_remove("\\$") %>%
    # remove all occurrences of commas
    str_remove_all(",") %>%
    # convert to numeric
    as.numeric()
}

# clean country/parent co and contributions 
contributions <- contributions %>%
  separate(country_of_origin_parent_company, 
           into = c("country", "parent"), 
           sep = "/", 
           extra = "merge") %>%
  mutate(
    total = parse_currency(total),
    dems = parse_currency(dems),
    repubs = parse_currency(repubs)
  )
```
-   Write a function called `scrape_pac()` that scrapes information from the Open Secrets webpage for foreign-connected PAC contributions in a given year. 
```{r}
scrape_pac <- function(url){
  #getting the data and cleaning 
  contrib <- url %>%
    read_html() %>% 
    html_nodes(css="table") %>% # this will isolate all tables on page
    html_table()
  #cleaning names
  contrib <- contrib[[1]] %>% 
    janitor::clean_names()
  #getting year from url
  year <- str_sub(url,start=-4L,end=-1L)
  #adding year column
  contrib <- contrib %>% 
    mutate('year'=as.numeric(year))
  return(contrib)
}

url<-"https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
scrape_pac(url)
```

-   Define the URLs for 2022, 2020, and 2000 contributions. Then, test your function using these URLs as inputs. Does the function seem to do what you expected it to do?
```{r}
#defining the URLs for 2022, 2020, and 2000 contributions
url00<-"https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"
url20<-"https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url22<-"https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
#checking the function for 2022, 2020, and 2000 contributions
scrape_pac(url00)
scrape_pac(url20)
scrape_pac(url22)
```
Yes it works as expected.

-   Construct a vector called `urls` that contains the URLs for each webpage that contains information on foreign-connected PAC contributions for a given year.
```{r}
#constructing the vector
y = as.numeric(format(Sys.Date(),"%Y"))
urls<-c()
for (i in 2001:y-1){
  url<-paste("https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/",
             as.character(i),sep="");
   urls <- append(urls,url)}
```

-   Map the `scrape_pac()` function over `urls` in a way that will result in a data frame called `contributions_all`.
```{r}
contributions_all<-map(urls,~scrape_pac(.))
#convert to dataframe
contributions_all <- do.call(rbind,contributions_all)
```

-   Write the data frame to a csv file called `contributions-all.csv` in the `data` folder.

```{r}
write.csv(contributions_all, "data/contributions-all.csv", row.names=FALSE)
```

# Scraping consulting jobs

```{r}
library(robotstxt)

base_url <- "https://www.consultancy.uk/jobs/page/1"
listings <- base_url %>%
  read_html()
#identifying css selectors

#job 
listings %>%
  html_elements(css = "#vacaturenaam")

#firm
listings %>%
  html_elements(css = "#bedrijf")

#functional area
listings %>%
  html_elements(css = "th.hide-tablet-and-less")

#type
listings %>%
  html_elements(css = "th.hide-tablet-landscape")
```


Can you get all pages of ads, and not just the first one, `https://www.consultancy.uk/jobs/page/1` into a dataframe?
yes already done.

-   Write a function called `scrape_jobs()` that scrapes information from the webpage for consulting positions. This function should

```
base_url <- "https://www.consultancy.uk/jobs/page/1"
url <- str_c(base_url, page)
```
I expect it to return this url:https://www.consultancy.uk/jobs/page/15. it combines the 2 strings
The url i should use to combine strings is https://www.consultancy.uk/jobs/page/

-   Construct a vector called `pages` that contains the numbers for each page available


-   Map the `scrape_jobs()` function over `pages` in a way that will result in a data frame called `all_consulting_jobs`.

-   Write the data frame to a csv file called `all_consulting_jobs.csv` in the `data` folder.


```{r}
#| label: consulting_jobs_url
#| eval: false
paths_allowed("https://www.consultancy.uk") #is it ok to scrape?

base_url <- "https://www.consultancy.uk/jobs/page/1"

#creating a vector with the urls
pages<-c()
for (i in 1:8){
  url<-paste("https://www.consultancy.uk/jobs/page/",
             as.character(i),sep="");
   pages <- append(pages,url)}

#scraping function
scrape_jobs <- function(url){
  #getting the data and cleaning 
  listings <- url %>%
    read_html() %>% 
    html_nodes(css="table") %>% # this will isolate all tables on page
    html_table()
  #cleaning names
  listings <- listings[[1]] %>% 
    janitor::clean_names()
  return(listings)
}

#defining a test URL for page 5
url5<-"https://www.consultancy.uk/jobs/page/5"
#checking the function for page 5 
jobscheck <- scrape_jobs(url5)

#gathering all data
jobs<-map(pages,~scrape_jobs(.))

#convert to dataframe
jobs <- do.call(rbind,jobs)

#writing the data frame to a csv
write.csv(jobs, "data/all_consulting_jobs.csv", row.names=FALSE)
```


# Create a shiny app 

We have already worked with the data on electricity production and usage, GDP/capita and CO2/capita since 1990.
You have to create a simple Shiny app, where a user chooses a country from a drop down list and a time interval between 1990 and 2020 and shiny outputs the following

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "electricity-shiny.png"), error = FALSE)
```

You can use chatGPT to get the basic layout of Shiny app, but you need to adjust the code it gives you. Ask chatGPT to create the Shiny app using the `gapminder` data and make up similar requests for the inputs/outpus you are thinking of deploying.



# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown (Rmd) file as a Word or HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing your changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: Nobody
-   Approximately how much time did you spend on this problem set: a lot 
-   What, if anything, gave you the most trouble: 

**Please seek out help when you need it,** and remember the [15-minute rule](https://dsb2023.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?
Yes

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
